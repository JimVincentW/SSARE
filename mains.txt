./SSARE/qdrant_service/main.py:
from fastapi import FastAPI, HTTPException
import httpx
from qdrant_client import QdrantClient
import json
from redis.asyncio import Redis
from sqlalchemy import update
from core.models import ProcessedArticleModel
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from fastapi.exceptions import RequestValidationError
from starlette.responses import JSONResponse
from core.models import ArticleBase

"""
This Service runs on port 6969 and is responsible qdrant related event-handling.
It is responsible for:
1. Creating embeddings jobs
2. Storing embeddings in Qdrant
3. Updating the flags in PostgreSQL for articles that have embeddings
4. [TODO] Querying Qdrant
"""


app = FastAPI()

qdrant_client = QdrantClient(host='qdrant_service', port=6333)
collection_name = 'articles'

@app.get("/healthcheck")
async def healthcheck():
    return {"message": "OK"}

# Add exception handler for RequestValidationError
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    return JSONResponse(
        status_code=400,
        content={"detail": exc.errors(), "body": exc.body},
    )


# get articles from postgres and create embeddings
@app.post("/create_embedding_jobs")
async def create_embeddings_jobs():
    """
    This function is triggered by an api. It reads from postgres /articles where embeddings_created = 0.
    It writes to redis queue 5 - channel articles_without_embedding_queue.
    It doesn't trigger the generate_embeddings function in nlp_service. That is done by the scheduler.
    """
    try:
        async with httpx.AsyncClient() as client:
            articles_without_embeddings = await client.get("http://postgres_service:5432/articles")
            articles_without_embeddings = articles_without_embeddings.json()

        redis_conn_unprocessed_articles = Redis(host='redis', port=6379, db=5)
        for article in articles_without_embeddings:
            await redis_conn_unprocessed_articles.lpush('articles_without_embedding_queue', json.dumps(article))

        return {"message": "Embedding jobs created."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
    

# get articles from postgres and create embeddings
@app.post("/store_embeddings")
async def store_embeddings():
    """
    This function is triggered by an api. It reads from redis queue 6 - channel articles_with_embeddings.
    It stores the embeddings in Qdrant.

    """
    try:
        redis_conn = await Redis(host='redis', port=6379, db=6)
        urls_to_update = []
        articles_with_embedding_json = await redis_conn.rpop('articles_with_embeddings')

        while True:
            article_with_embedding = json.loads(articles_with_embedding_json)
            validated_article = ArticleBase(**article_with_embedding)

            payload = {
                "headline": article_with_embedding["headline"],
                "text": " ".join(article_with_embedding["paragraphs"]),  # Combine paragraphs into a single text
                "source": article_with_embedding["source"],
                "url": article_with_embedding["url"],
            }

            qdrant_client.upsert(
                collection_name=collection_name,
                points=[{
                    "id": article_with_embedding["url"],  # Use URL as unique identifier
                    "vector": article_with_embedding["embeddings"],
                    "payload": payload
                }]
            )
            urls_to_update.append(validated_article.url)
            
            async with httpx.AsyncClient() as client:
                await client.post("http://postgres_service:5432/update_qdrant_flags", json={"urls": urls_to_update})

        return {"message": "Embeddings processed and stored in Qdrant."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

./SSARE/core/models.py:
from pydantic import BaseModel, Field
from typing import List, Optional

class ArticleBase(BaseModel):
    url: str = Field(...)
    headline: str = Field(...)
    paragraphs: str = Field(...)
    source: Optional[str] = None
    embeddings: Optional[List[float]] = None

    class Config:
        orm_mode = True

class ArticleModel(BaseModel):
    url: str = Field(..., description="The URL of the article")
    headline: str = Field(..., description="The headline of the article")
    paragraphs: str = Field(..., description="The paragraphs of the article")
    source: Optional[str] = Field(None, description="The source of the article")
    embeddings: Optional[List[float]] = Field(None, description="The vector embeddings of the article's content")
    embeddings_created: Optional[bool] = Field(False, description="Flag to indicate if embeddings are created")
    isStored_in_qdrant: Optional[bool] = Field(False, description="Flag to indicate if the article is stored in Qdrant")

    class Config:
        orm_mode = True
        schema_extra = {
            "example": {
                "url": "https://example-news.com/article1",
                "headline": "Breaking News: Example Event Occurs",
                "paragraphs": "Example paragraph 1. Example paragraph 2. Example paragraph 3",
                "source": "Example News",
                "embeddings": [0.01, 0.02, ..., 0.05],
                "embeddings_created": True,
                "isStored_in_qdrant": True
            }
        }



./SSARE/postgres_service/main.py:
from pydantic import BaseModel
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException, Query
from fastapi.encoders import jsonable_encoder
from sqlalchemy import create_engine, select, update, bindparam
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.orm import sessionmaker
from sqlalchemy import Column, Integer, String, Text
from redis.asyncio import Redis 
import json
from contextlib import asynccontextmanager
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from core.models import ArticleBase, ArticleModel
from core.utils import load_config
from pydantic import ValidationError

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


Base = declarative_base()

# SQLAlchemy model - unprocessed articles
class ArticleModel(Base):
    """
    This model is used to store unprocessed articles in PostgreSQL after scraping.
    """
    __tablename__ = 'articles'
    url = Column(String, primary_key=True, index=True)
    headline = Column(String)
    paragraphs = Column(Text)
    source = Column(String, nullable=True)
    embedding = Column(Text)  # Storing the embedding as JSON

    embeddings_created = Column(Integer, default=0)  # 0 = False, 1 = True
    isStored_in_qdrant = Column(Integer, default=0)  # 0 = False, 1 = True

    def model_dump(self):
        return {
            "url": self.url,
            "headline": self.headline,
            "paragraphs": json.loads(self.paragraphs),
            "source": self.source,
            "embedding": json.loads(self.embedding),
            "embeddings_created": self.embeddings_created,
            "isStored_in_qdrant": self.isStored_in_qdrant
        }


# SQLAlchemy model
class ProcessedArticleModel(Base):
    """
    This model is used to store processed articles in PostgreSQL after NLP processing.
    """
    __tablename__ = 'processed_articles'
    url = Column(String, primary_key=True, index=True)
    headline = Column(String)
    paragraphs = Column(Text)
    source = Column(String, nullable=True)
    embedding = Column(Text)  # Storing the embedding as JSON

    embeddings_created = Column(Integer, default=0)  # 0 = False, 1 = True
    isStored_in_qdrant = Column(Integer, default=0)  # 0 = False, 1 = True

    def model_dump(self):
        return {
            "url": self.url,
            "headline": self.headline,
            "paragraphs": json.loads(self.paragraphs),
            "source": self.source,
            "embedding": json.loads(self.embedding),
            "embeddings_created": self.embeddings_created,
            "isStored_in_qdrant": self.isStored_in_qdrant
        }


app = FastAPI()

redis_conn_flags = Redis(host='redis', port=6379, db=0)  # For flags
redis_conn_articles = Redis(host='redis', port=6379, db=2)  # For articles

async def setup_db_connection():
    config = load_config()['postgresql']
    database_name = config['postgres_db']
    table_name = config['postgres_table_name']
    user = config['postgres_user']
    password = config['postgres_password']
    host = config['postgres_host']

    engine = create_async_engine(f'postgresql+asyncpg://{user}:{password}@{host}/{database_name}?ssl=False')
    return engine

async def close_db_connection(engine):
    # Close PostgreSQL connection
    await engine.dispose()

@asynccontextmanager
async def db_lifespan(app: FastAPI):
    """
    This function is used to setup and close the PostgreSQL connection.
    It is used as a context manager.
    """
    # Before app startup
    engine = await setup_db_connection()
    app.state.db = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)
    yield
    # After app shutdown
    await close_db_connection(engine)

app = FastAPI(lifespan=db_lifespan)

@app.get("/flags")
async def produce_flags():
    """
    This function produces flags for the scraper service. It is triggered by an API call.
    It deletes all existing flags in Redis Queue 0 - channel "scrape_sources" and pushes new flags.
    The flag creation mechanism is to be updated, and not hardcoded like now.
    """
    redis_conn_flags.delete("scrape_sources")
    flags = ["cnn",]
    for flag in flags:
        await redis_conn_flags.lpush("scrape_sources", flag)
    return {"message": f"Flags produced: {', '.join(flags)}"}

@app.get('/articles', response_model=List[ArticleBase])
async def get_articles(
    embeddings_created: Optional[bool] = Query(None),
    isStored_in_Qdrant: Optional[bool] = Query(None),
    skip: int = 0,
    limit: int = 10
    ):
    """
    This function is used to retrieve articles from PostgreSQL.
    It can be used to retrieve all articles, or articles with specific flags.
    """
    async with app.state.db() as session:
        query = select(ArticleModel)
        
        if embeddings_created is not None:
            query = query.where(ArticleModel.embeddings_created == embeddings_created)

        if isStored_in_Qdrant is not None:
            query = query.where(ArticleModel.isStored_in_qdrant == isStored_in_Qdrant)

        query = query.offset(skip).limit(limit)
        result = await session.execute(query)
        articles = result.scalars().all()
        
        return jsonable_encoder(articles)

@app.post("/store_raw_articles")
async def store_raw_articles():
    """
    This function is triggered by an API call. It reads from redis queue 1 - channel raw_articles_queue
    and stores the articles in PostgreSQL.
    """
    try:
        redis_conn = await Redis(host='redis', port=6379, db=1)
        raw_articles = await redis_conn.lrange('raw_articles_queue', 0, -1)
        await redis_conn.delete('raw_articles_queue')

        async with app.state.db() as session:
            for raw_article in raw_articles:
                try:
                    article_data = json.loads(raw_article)
                    article = ArticleBase(**article_data)
                    db_article = ArticleModel(**article.model_dump())
                    session.add(db_article)
                except ValidationError as e:
                    logger.error(f"Validation error for article: {e}")
                    # You can also log article_data to see what's wrong
                except json.JSONDecodeError as e:
                    logger.error(f"JSON decoding error: {e}")
                    # Log raw_article here to inspect the malformed JSON

            await session.commit()

        return {"message": "Raw articles stored successfully."}
    except Exception as e:
        logger.error(f"Error storing articles: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/store_articles_with_embeddings")
async def store_processed_articles():
    """
    This function is triggered by an API call. 
    It reads from redis queue 6 - channel articles_with_embeddings
    and stores the articles in PostgreSQL.
    """
    try:
        redis_conn = await Redis(host='redis', port=6379, db=3)
        articles_with_embeddings = await redis_conn.lrange('articles_with_embeddings', 0, -1)
        await redis_conn.delete('articles_with_embeddings')

        articles = []
        embeddings = []
        async with app.state.db() as session:
            for article, embedding in zip(articles, embeddings):
                # Convert embedding to JSON
                embedding_json = json.dumps(embedding)
                # Create a new ProcessedArticleModel instance
                processed_article = ProcessedArticleModel(
                    url=article.url,
                    headline=article.headline,
                    paragraphs=json.dumps(article.paragraphs),  # Convert list to JSON
                    source=article.source,
                    embedding=embedding_json,
                    embeddings_created=1,  # Set to True
                )
                session.add(processed_article)
            await session.commit()

        return {"message": "Articles with embeddings stored successfully in PostgreSQL."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"An error occurred: {str(e)}")

@app.post("/update_qdrant_flags")
async def update_qdrant_flags(urls: List[str]):
    """
    This function is triggered by an API call.
    It updates the isStored_in_qdrant flag for articles in PostgreSQL which have been stored in Qdrant.
    It is used by the qdrant_service.
    """
    try:
        async with app.state.db() as session:
            stmt = update(ProcessedArticleModel).\
                where(ProcessedArticleModel.url == bindparam('url')).\
                values(isStored_in_qdrant=True)
            await session.execute(stmt, [{"url": url} for url in urls])
            await session.commit()

        return {"message": "Qdrant flags updated successfully."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


./SSARE/app/main.py:
from fastapi import FastAPI, HTTPException
import httpx
import os
from core.utils import load_config

app = FastAPI()

config = load_config()["postgresql"]


@app.get("/healthcheck")
async def healthcheck():
    return {"message": "OK"}

@app.post("/full_run")
async def full_run():
    try:
        # Produce flags
        await httpx.post("http://scraper_service:5432/flags")
        # Scrape data
        await httpx.post("http://scraper_service:8081/create_scrape_jobs")
        # Create embeddings
        await httpx.post("http://nlp_service:0420/create_embeddings")
        # Store embeddings
        await httpx.post("http://postgres_service:8000/store_embeddings")
        return {"message": "Full run complete."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

./SSARE/scraper_service/celery_worker.py:
from celery import Celery
import json
from celery.utils.log import get_task_logger
from core.models import ArticleBase
import pandas as pd
import subprocess
import logging
from redis import Redis
from pydantic import ValidationError

""" 
This Script is creating Celery tasks for scraping data from news sources.
It is triggered by the orchestrator service.
The scrapa_daa_task function reads from Redis Queue 0 - channel "scrape_sources" and creates a scraping job for each flag.
It passes a flag as a string argument to the scrape_single_source function.
"""


logging.basicConfig(level=logging.INFO)
logger = get_task_logger(__name__)

celery_app = Celery("worker", backend="redis://redis:6379/9", broker="redis://redis:6379/9")

@celery_app.task
def scrape_data_task():
    """
    This function will be called by the main.py script. It will check the flags in Redis Queue 0 - channel "scrape_sources"
    and create a scraping job for each flag with Celery.
    It passes a flag as a string argument to the scrape_single_source function.
    """
    logger.info("Received request to scrape data")
    try:
        # Synchronous Redis connection for flags
        redis_conn_flags = Redis(host='redis', port=6379, db=0)

        # Retrieve all flags from Redis
        flags = redis_conn_flags.lrange('scrape_sources', 0, -1)
        flags = [flag.decode('utf-8') for flag in flags]
        logger.info(f"Scraping data for {flags}")

        # Trigger scraping for each flag
        for flag in flags:
            scrape_single_source.delay(flag)
            logger.info(f"Scraping data for {flag} complete")
        logger.info("Scraping complete")
    except Exception as e:
        logger.error(f"Error in scraping data: {e}")
        raise e

@celery_app.task
def scrape_single_source(flag: str):
    """
    This function is triggered by the scrape_data_task function. It will run the corresponding scraper script
    for the flag. It will then read the CSV file created by the scraper script 
    and push the data to Redis Queue 1 - channel "raw_articles_queue".
    """
    logger.info(f"Single source scraping for {flag}")
    try:
        # Load scraper configuration from JSON file
        with open("./scrapers/scrapers_config.json") as file:
            config_json = json.load(file)

        # Check if the flag has a corresponding scraper configuration
        if flag not in config_json["scrapers"]:
            logger.error(f"No configuration found for flag: {flag}")
            return

        # Get the location of the scraper script
        script_location = config_json["scrapers"][flag]["location"]
        logger.info(f"Running script for {flag}")

        # Run the scraper script as a subprocess
        result = subprocess.run(["python", script_location], capture_output=True, text=True)
        if result.returncode != 0:
            logger.error(f"Error running script for {flag}: {result.stderr}")
            return

        # Read the scraped data from CSV to a DataFrame
        df = pd.read_csv(f"/app/scrapers/data/dataframes/{flag}_articles.csv")
        logger.info(df.head(3))


        # Add a 'source' column to the DataFrame with the flag
        df["source"] = flag
        articles = df.to_dict(orient="records")

        # Synchronous Redis connection for articles
        redis_conn_articles = Redis(host='redis', port=6379, db=1)


        # Validate and push articles to Redis
        for article_data in articles:
            try:
                validated_article = ArticleBase(**article_data)
                redis_conn_articles.lpush("raw_articles_queue", json.dumps(validated_article.model_dump()))
            except ValidationError as e:
                logger.error(f"Validation error for article: {e}")


        logger.info(f"Scraping for {flag} complete")
    except Exception as e:
        logger.error(f"Error in scraping data for {flag}: {e}")
        raise e

./SSARE/scraper_service/main.py:
from fastapi import FastAPI, HTTPException, Query
import requests
from pydantic import BaseModel
from typing import List
import importlib
import json
from fastapi import Body
from celery_worker import scrape_data_task
from core.utils import load_config
from redis.asyncio import Redis
from contextlib import asynccontextmanager
import logging
from core.models import ArticleBase

""""
This Service runs on port 8081 and is responsible for scraping articles.

"""



logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()


class Flag(BaseModel):
    flag: str




async def setup_redis_connection():
    # Setup Redis connection
    return await Redis(host='redis', port=6379, db=1, decode_responses=True)
async def close_redis_connection(redis_conn):

    # Close Redis connection
    await redis_conn.close()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Before app startup
    app.state.redis = await setup_redis_connection()
    yield
    # After app shutdown
    await close_redis_connection(app.state.redis)

app = FastAPI(lifespan=lifespan)


def get_scraper_config():
    """
    This function loads the scraper configuration from the JSON file.
    """
    with open("scrapers/scrapers_config.json") as f:
        return json.load(f)

@app.post("/create_scrape_jobs")
async def create_scrape_jobs():
    """
    This function is triggered by an API call from the orchestration container.
    It reads from Redis Queue 0 - channel "scrape_sources" and creates a scraping job for each flag.
    It passes a flag as a string argument to the scrape_single_source function.
    It validates the flags against the scraper configuration.
    The scrape job itself is created by triggering the scrape_data_task function in celery_worker.py.
    -> inside of celery_worker.py:
        The scrape_data_task function reads from the redis queue and create scraping jobs for each flag
        by triggering the scrape_single_source function.
        When the scrape_single_source function is complete, 
        it will push the data to Redis Queue 1 - channel "raw_articles_queue".

    """
    redis_conn_flags = await Redis(host='redis', port=6379, db=0)  # For flags
    logger.info("Creating scrape jobs")
    flags = await redis_conn_flags.lrange('scrape_sources', 0, -1)
    flags = [flag.decode('utf-8') for flag in flags]

    config_json = get_scraper_config()
    if not all(flag in config_json["scrapers"].keys() for flag in flags):
        raise HTTPException(status_code=400, detail="Invalid flags provided.")
    logger.info("Scrape jobs created")
    result = scrape_data_task.delay()
    logger.info(f"Scrape data task created with ID: {result.id}")
    return {"message": "Scraping triggered successfully."}
        

@app.get("/health")
def health_check():
    """Health check endpoint"""
    return {"status": "ok"}




./SSARE/nlp_service/main.py:
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from typing import List
from redis.asyncio import Redis
from core.models import ArticleBase
import json
from pydantic import ValidationError
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

"""
This Service runs on port 0420 and is responsible for generating embeddings for articles.
"""

app = FastAPI()

model = SentenceTransformer("all-MiniLM-L6-v2")

@app.post("/generate_embeddings")
async def generate_embeddings():
    """
    This function generates embeddings for articles that do not have embeddings.
    It is triggered by an API call from the orchestration container. 
    It reads from redis queue 5 - channel articles_without_embedding_queue.
    It writes to redis queue 6 - channel articles_with_embeddings.
    """
    try:
        redis_conn_raw = await Redis(host='redis', port=6379, db=5)
        redis_conn_processed = await Redis(host='redis', port=6379, db=6)

        # Retrieve articles from Redis Queue 5
        raw_articles_json = await redis_conn_raw.lrange('articles_without_embedding_queue', 0, -1)

        for raw_article_json in raw_articles_json:
            try:
                # Decode and load the article
                raw_article = json.loads(raw_article_json.decode('utf-8'))
                article = ArticleBase(**raw_article)

                # Generate embeddings
                embedding = model.encode(article.headline + " ".join(article.paragraphs)).tolist()
                article_with_embedding = article.dict()
                article_with_embedding["embeddings"] = embedding

                # Push to Redis Queue 6
                await redis_conn_processed.lpush('articles_with_embeddings', json.dumps(article_with_embedding))
            except (json.JSONDecodeError, ValidationError) as e:
                logger.error(f"Error processing article: {e}")

        return {"message": "Embeddings generated"}
    except Exception as e:
        logger.error(f"Error in generating embeddings: {e}")
        raise HTTPException(status_code=500, detail=str(e))



