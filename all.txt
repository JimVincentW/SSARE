./docker-compose.yml:
version: '3.9'

services:
  redis:
    image: redis:latest
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./core/configs/redis.conf:/usr/local/etc/redis/redis.conf
    ports:
      - "6379:6379"
    networks:
      - app_network

  main_core_app:
    build: 
      context: ./SSARE
      dockerfile: ./app/Dockerfile
    ports:
      - "8080:8080"
    networks:
      - app_network
    depends_on:
      - scraper_service
      - postgres_service
      - redis
    develop:
      watch:
        - action: sync
          path: .
          target: /app
        - action: rebuild
          path: .

  scraper_service:
    build: 
      context: ./SSARE
      dockerfile: ./scraper_service/Dockerfile
    ports:
      - "8081:8081"
    networks:
      - app_network
    develop:
      watch:
        - action: sync
          path: .
          target: /app
        - action: rebuild
          path: .
    depends_on:
      - redis
  
  qdrant_service:
    build: 
      context: ./SSARE
      dockerfile: ./qdrant_service/Dockerfile
    ports:
      - "6969:6"
    networks:
      - app_network
    develop:
      watch:
        - action: sync
          path: .
          target: /app
        - action: rebuild
          path: .
      
  qdrant_storage:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    networks:
      - app_network

  postgres_db:
    image: postgres:latest
    environment:
      POSTGRES_DB: root
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    volumes:
      - ./sql_commands:/docker-entrypoint-initdb.d
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - app_network

  postgres_service:
    build: 
      context: ./SSARE
      dockerfile: ./postgres_service/Dockerfile
    volumes:
      - ./sql_commands:/docker-entrypoint-initdb.d
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: root
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    ports:
      - "5434:5434"
    networks:
      - app_network
    develop:
      watch:
        - action: sync
          path: .
          target: /app
          ignore: 
            - requirements.txt
        - action: rebuild
          path: .
    depends_on:
      - redis
      - postgres_db



      
  nlp_service:
    build: 
      context: ./SSARE
      dockerfile: ./nlp_service/Dockerfile
    ports:
      - "0420:0420"
    networks:
      - app_network
    depends_on:
      - redis
      - postgres_service
    develop:
      watch:
        - action: sync
          path: .
          target: /app
        - action: rebuild
          path: .


networks:
  app_network:

volumes:
  postgres_data:


./SSARE/qdrant_service/Dockerfile:
FROM python:3.9

WORKDIR /app

# Copy the requirements.txt file
COPY qdrant_service/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the core directory
COPY core ./core

# Copy the main.py and other files from postgres_service
COPY qdrant_service/main.py .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "6969"]


./SSARE/qdrant_service/main.py:
from fastapi import FastAPI, HTTPException
import httpx
from qdrant_client import QdrantClient
import json
from redis.asyncio import Redis
from sqlalchemy import update
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from fastapi.exceptions import RequestValidationError
from starlette.responses import JSONResponse
from core.models import ArticleBase

"""
This Service runs on port 6969 and is responsible qdrant related event-handling.
It is responsible for:
1. Creating embeddings jobs
2. Storing embeddings in Qdrant
3. Updating the flags in PostgreSQL for articles that have embeddings
4. [TODO] Querying Qdrant
"""


app = FastAPI()

qdrant_client = QdrantClient(host='qdrant_service', port=6333)
collection_name = 'articles'

@app.get("/healthcheck")
async def healthcheck():
    return {"message": "OK"}

# Add exception handler for RequestValidationError
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    return JSONResponse(
        status_code=400,
        content={"detail": exc.errors(), "body": exc.body},
    )


# get articles from postgres and create embeddings
@app.post("/create_embedding_jobs")
async def create_embeddings_jobs():
    """
    This function is triggered by an api. It reads from postgres /articles where embeddings_created = 0.
    It writes to redis queue 5 - channel articles_without_embedding_queue.
    It doesn't trigger the generate_embeddings function in nlp_service. That is done by the scheduler.
    """
    try:
        async with httpx.AsyncClient() as client:
            articles_without_embeddings = await client.get("http://postgres_service:5434/articles")
            articles_without_embeddings = articles_without_embeddings.json()

        redis_conn_unprocessed_articles = Redis(host='redis', port=6379, db=5)
        for article in articles_without_embeddings:
            await redis_conn_unprocessed_articles.lpush('articles_without_embedding_queue', json.dumps(article))

        return {"message": "Embedding jobs created."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
    

# get articles from postgres and create embeddings
@app.post("/store_embeddings")
async def store_embeddings():
    """
    This function is triggered by an api. It reads from redis queue 6 - channel articles_with_embeddings.
    It stores the embeddings in Qdrant.

    """
    try:
        redis_conn = await Redis(host='redis', port=6379, db=6)
        urls_to_update = []
        articles_with_embedding_json = await redis_conn.rpop('articles_with_embeddings')

        while True:
            article_with_embedding = json.loads(articles_with_embedding_json)
            validated_article = ArticleBase(**article_with_embedding)

            payload = {
                "headline": article_with_embedding["headline"],
                "text": " ".join(article_with_embedding["paragraphs"]),  # Combine paragraphs into a single text
                "source": article_with_embedding["source"],
                "url": article_with_embedding["url"],
            }

            qdrant_client.upsert(
                collection_name=collection_name,
                points=[{
                    "id": article_with_embedding["url"],  # Use URL as unique identifier
                    "vector": article_with_embedding["embeddings"],
                    "payload": payload
                }]
            )
            urls_to_update.append(validated_article.url)
            
            async with httpx.AsyncClient() as client:
                await client.post("http://postgres_service:5434/update_qdrant_flags", json={"urls": urls_to_update})

        return {"message": "Embeddings processed and stored in Qdrant."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

./SSARE/core/models.py:
from pydantic import BaseModel, Field
from typing import List, Optional

class ArticleBase(BaseModel):
    url: str = Field(...)
    headline: str = Field(...)
    paragraphs: str = Field(...)
    source: Optional[str] = None
    embeddings: Optional[List[float]] = None

    class Config:
        orm_mode = True

class ArticleModel(BaseModel):
    url: str = Field(..., description="The URL of the article")
    headline: str = Field(..., description="The headline of the article")
    paragraphs: str = Field(..., description="The paragraphs of the article")
    source: Optional[str] = Field(None, description="The source of the article")
    embeddings: Optional[List[float]] = Field(None, description="The vector embeddings of the article's content")
    embeddings_created: Optional[bool] = Field(False, description="Flag to indicate if embeddings are created")
    isStored_in_qdrant: Optional[bool] = Field(False, description="Flag to indicate if the article is stored in Qdrant")

    class Config:
        orm_mode = True
        schema_extra = {
            "example": {
                "url": "https://example-news.com/article1",
                "headline": "Breaking News: Example Event Occurs",
                "paragraphs": "Example paragraph 1. Example paragraph 2. Example paragraph 3",
                "source": "Example News",
                "embeddings": [0.01, 0.02, ..., 0.05],
                "embeddings_created": True,
                "isStored_in_qdrant": True
            }
        }



./SSARE/postgres_service/Dockerfile:
FROM python:3.9

WORKDIR /app

# Copy the requirements.txt file
COPY postgres_service/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the core directory
COPY core ./core

# Copy the main.py and other files from postgres_service
COPY postgres_service/main.py .
COPY postgres_service/sql_commands ./sql_commands

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "5434"]


./SSARE/postgres_service/main.py:
from pydantic import BaseModel, ValidationError
from typing import List, Optional
from fastapi import FastAPI, HTTPException, Query, Request
import json
import psycopg2
from psycopg2.pool import SimpleConnectionPool
from contextlib import contextmanager
from redis.asyncio import Redis 
import logging
from core.utils import load_config

# Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models
class ArticleModel(BaseModel):
    url: str
    headline: str
    paragraphs: str  # JSON string
    source: Optional[str]
    embedding: Optional[str]  # JSON string
    embeddings_created: int = 0  # 0 = False, 1 = True
    isStored_in_qdrant: int = 0  # 0 = False, 1 = True

class ProcessedArticleModel(ArticleModel):
    pass

# Initialize the connection pool
def init_db_pool():
    config = load_config()['postgresql']
    return SimpleConnectionPool(minconn=1, maxconn=10,
                                dbname=config["postgres_db"],
                                user=config["postgres_user"],
                                password=config["postgres_password"],
                                host=config["postgres_host"])

pool = init_db_pool()

@contextmanager
def get_db_connection():
    conn = pool.getconn()
    try:
        yield conn
    finally:
        pool.putconn(conn)

# FastAPI app initialization
app = FastAPI()

# Redis connections
redis_conn_flags = Redis(host='redis', port=6379, db=0)  # For flags
redis_conn_articles = Redis(host='redis', port=6379, db=2)  # For articles

@app.get("/flags")
async def produce_flags():
    await redis_conn_flags.delete("scrape_sources")
    flags = ["cnn"]
    for flag in flags:
        await redis_conn_flags.lpush("scrape_sources", flag)
    return {"message": f"Flags produced: {', '.join(flags)}"}

@app.get('/articles', response_model=List[ArticleModel])
async def get_articles(embeddings_created: Optional[bool] = Query(None), isStored_in_Qdrant: Optional[bool] = Query(None), skip: int = 0, limit: int = 10):
    with get_db_connection() as conn:
        cur = conn.cursor()
        query = "SELECT * FROM articles"
        conditions = []
        if embeddings_created is not None:
            conditions.append(f"embeddings_created = {embeddings_created}")
        if isStored_in_Qdrant is not None:
            conditions.append(f"isStored_in_qdrant = {isStored_in_Qdrant}")
        if conditions:
            query += " WHERE " + " AND ".join(conditions)
        query += f" OFFSET {skip} LIMIT {limit}"

        cur.execute(query)
        articles = cur.fetchall()
        cur.close()
        return articles

@app.post("/store_raw_articles")
async def store_raw_articles():
    try:
        redis_conn = await Redis(host='redis', port=6379, db=1)
        logger.info("Connected to Redis")
        raw_articles = await redis_conn.lrange('raw_articles_queue', 0, -1)
        logger.info("Retrieved raw articles from Redis")
        
        with get_db_connection() as conn:
            cur = conn.cursor()
            batch_size = 50
            for i in range(0, len(raw_articles), batch_size):
                batch = raw_articles[i:i + batch_size]
                for raw_article in batch:
                    try:
                        article_data = json.loads(raw_article)
                        article = ArticleModel(**article_data)
                        insert_query = "INSERT INTO articles (url, headline, paragraphs, source, embedding, embeddings_created, isStored_in_qdrant) VALUES (%s, %s, %s, %s, %s, %s, %s)"
                        cur.execute(insert_query, (article.url, article.headline, article.paragraphs, article.source, article.embedding, article.embeddings_created, article.isStored_in_qdrant))
                    except ValidationError as e:
                        logger.error(f"Validation error for article: {e}")
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON decoding error: {e}")
                conn.commit()
            cur.close()
        return {"message": "Raw articles stored successfully."}
    except Exception as e:
        logger.error(f"Error storing articles: {e}")
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/store_articles_with_embeddings")
async def store_processed_articles():
    try:
        redis_conn = await Redis(host='redis', port=6379, db=6)
        articles_with_embeddings = await redis_conn.lrange('articles_with_embeddings', 0, -1)
        await redis_conn.delete('articles_with_embeddings')

        with get_db_connection() as conn:
            cur = conn.cursor()
            batch_size = 10  # Adjust the batch size as needed
            for i in range(0, len(articles_with_embeddings), batch_size):
                batch = articles_with_embeddings[i:i + batch_size]
                for article_with_embedding in batch:
                    try:
                        article_data = json.loads(article_with_embedding)
                        article = ProcessedArticleModel(**article_data)
                        insert_query = "INSERT INTO processed_articles (url, headline, paragraphs, source, embedding, embeddings_created, isStored_in_qdrant) VALUES (%s, %s, %s, %s, %s, %s, %s)"
                        cur.execute(insert_query, (article.url, article.headline, article.paragraphs, article.source, article.embedding, article.embeddings_created, article.isStored_in_qdrant))
                    except ValidationError as e:
                        logger.error(f"Validation error for article: {e}")
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON decoding error: {e}")
                conn.commit()
            cur.close()
        return {"message": "Articles with embeddings stored successfully in PostgreSQL."}
    except Exception as e:
        logger.error(f"Error storing articles: {e}")
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/update_qdrant_flags")
async def update_qdrant_flags(urls: List[str]):
    try:
        with get_db_connection() as conn:
            cur = conn.cursor()
            update_query = "UPDATE processed_articles SET isStored_in_qdrant = 1 WHERE url = %s"
            for url in urls:
                cur.execute(update_query, (url,))
            conn.commit()
            cur.close()
        return {"message": "Qdrant flags updated successfully."}
    except Exception as e:
        logger.error(f"Error updating Qdrant flags: {e}")
        raise HTTPException(status_code=400, detail=str(e))

./SSARE/app/Dockerfile:
FROM python:3.9

WORKDIR /app

# Copy the requirements.txt file
COPY app/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the core directory
COPY core ./core

# Copy the main.py file
COPY app/main.py .

# Copy the config.ini file
COPY app/config.ini .

EXPOSE 8080

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]


./SSARE/app/config.ini:
[server]
host = 0.0.0.0
port = 8000
log_level = DEBUG

[open_politics]
# Qdrant Settings
qdrant_host = localhost
qdrant_grpc_port = 6334
qdrant_prefer_grpc = True
qdrant_collection_name = open_politics_live_articles

# Postgres Settings
[postgresql]
postgres_db = root
postgres_user = admin
postgres_password = password
postgres_host = postgres_service
postgres_table_name = unprocessed_articles

./SSARE/app/main.py:
from fastapi import FastAPI, HTTPException
import httpx
import os
from core.utils import load_config

app = FastAPI()

config = load_config()["postgresql"]


@app.get("/healthcheck")
async def healthcheck():
    return {"message": "OK"}

@app.post("/full_run")
async def full_run():
    try:
        # Produce flags
        await httpx.post("http://scraper_service:5432/flags")
        # Scrape data
        await httpx.post("http://scraper_service:8081/create_scrape_jobs")
        # Create embeddings
        await httpx.post("http://nlp_service:0420/create_embeddings")
        # Store embeddings
        await httpx.post("http://postgres_service:8000/store_embeddings")
        return {"message": "Full run complete."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

./SSARE/scraper_service/Dockerfile:
FROM python:3.9

WORKDIR /app

# Copy the requirements.txt file
COPY scraper_service/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
RUN apt-get update && apt-get install -y supervisor

# Copy the core directory
COPY core ./core

# Copy the main.py and other files from scraper_service
COPY scraper_service/main.py .
COPY scraper_service/scrapers ./scrapers
COPY scraper_service/scrapers/scrapers_config.json .
COPY scraper_service/celery_worker.py .

# Copy the supervisord.conf file
COPY core/configs/supervisord.conf /etc/supervisor/conf.d/supervisord.conf

CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"]



./SSARE/scraper_service/celery_worker.py:
from celery import Celery
import json
from celery.utils.log import get_task_logger
from core.models import ArticleBase
import pandas as pd
import subprocess
import logging
from redis import Redis
from pydantic import ValidationError

""" 
This Script is creating Celery tasks for scraping data from news sources.
It is triggered by the orchestrator service.
The scrapa_daa_task function reads from Redis Queue 0 - channel "scrape_sources" and creates a scraping job for each flag.
It passes a flag as a string argument to the scrape_single_source function.
"""


logging.basicConfig(level=logging.INFO)
logger = get_task_logger(__name__)

celery_app = Celery("worker", backend="redis://redis:6379/9", broker="redis://redis:6379/9")

@celery_app.task
def scrape_data_task():
    """
    This function will be called by the main.py script. It will check the flags in Redis Queue 0 - channel "scrape_sources"
    and create a scraping job for each flag with Celery.
    It passes a flag as a string argument to the scrape_single_source function.
    """
    logger.info("Received request to scrape data")
    try:
        # Synchronous Redis connection for flags
        redis_conn_flags = Redis(host='redis', port=6379, db=0)

        # Retrieve all flags from Redis
        flags = redis_conn_flags.lrange('scrape_sources', 0, -1)
        flags = [flag.decode('utf-8') for flag in flags]
        logger.info(f"Scraping data for {flags}")

        # Trigger scraping for each flag
        for flag in flags:
            scrape_single_source.delay(flag)
            logger.info(f"Scraping data for {flag} complete")
        logger.info("Scraping complete")
    except Exception as e:
        logger.error(f"Error in scraping data: {e}")
        raise e

@celery_app.task
def scrape_single_source(flag: str):
    """
    This function is triggered by the scrape_data_task function. It will run the corresponding scraper script
    for the flag. It will then read the CSV file created by the scraper script 
    and push the data to Redis Queue 1 - channel "raw_articles_queue".
    """
    logger.info(f"Single source scraping for {flag}")
    try:
        # Load scraper configuration from JSON file
        with open("./scrapers/scrapers_config.json") as file:
            config_json = json.load(file)

        # Check if the flag has a corresponding scraper configuration
        if flag not in config_json["scrapers"]:
            logger.error(f"No configuration found for flag: {flag}")
            return

        # Get the location of the scraper script
        script_location = config_json["scrapers"][flag]["location"]
        logger.info(f"Running script for {flag}")

        # Run the scraper script as a subprocess
        result = subprocess.run(["python", script_location], capture_output=True, text=True)
        if result.returncode != 0:
            logger.error(f"Error running script for {flag}: {result.stderr}")
            return

        # Read the scraped data from CSV to a DataFrame
        df = pd.read_csv(f"/app/scrapers/data/dataframes/{flag}_articles.csv")
        logger.info(df.head(3))


        # Add a 'source' column to the DataFrame with the flag
        df["source"] = flag
        articles = df.to_dict(orient="records")

        # Synchronous Redis connection for articles
        redis_conn_articles = Redis(host='redis', port=6379, db=1)


        # Validate and push articles to Redis
        for article_data in articles:
            try:
                validated_article = ArticleBase(**article_data)
                redis_conn_articles.lpush("raw_articles_queue", json.dumps(validated_article.model_dump()))
            except ValidationError as e:
                logger.error(f"Validation error for article: {e}")


        logger.info(f"Scraping for {flag} complete")
    except Exception as e:
        logger.error(f"Error in scraping data for {flag}: {e}")
        raise e

./SSARE/scraper_service/main.py:
from fastapi import FastAPI, HTTPException, Query
import requests
from pydantic import BaseModel
from typing import List
import importlib
import json
from fastapi import Body
from celery_worker import scrape_data_task
from core.utils import load_config
from redis.asyncio import Redis
from contextlib import asynccontextmanager
import logging
from core.models import ArticleBase

""""
This Service runs on port 8081 and is responsible for scraping articles.

"""



logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()


class Flag(BaseModel):
    flag: str




async def setup_redis_connection():
    # Setup Redis connection
    return await Redis(host='redis', port=6379, db=1, decode_responses=True)
async def close_redis_connection(redis_conn):

    # Close Redis connection
    await redis_conn.close()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Before app startup
    app.state.redis = await setup_redis_connection()
    yield
    # After app shutdown
    await close_redis_connection(app.state.redis)

app = FastAPI(lifespan=lifespan)


def get_scraper_config():
    """
    This function loads the scraper configuration from the JSON file.
    """
    with open("scrapers/scrapers_config.json") as f:
        return json.load(f)

@app.post("/create_scrape_jobs")
async def create_scrape_jobs():
    """
    This function is triggered by an API call from the orchestration container.
    It reads from Redis Queue 0 - channel "scrape_sources" and creates a scraping job for each flag.
    It passes a flag as a string argument to the scrape_single_source function.
    It validates the flags against the scraper configuration.
    The scrape job itself is created by triggering the scrape_data_task function in celery_worker.py.
    -> inside of celery_worker.py:
        The scrape_data_task function reads from the redis queue and create scraping jobs for each flag
        by triggering the scrape_single_source function.
        When the scrape_single_source function is complete, 
        it will push the data to Redis Queue 1 - channel "raw_articles_queue".

    """
    redis_conn_flags = await Redis(host='redis', port=6379, db=0)  # For flags
    logger.info("Creating scrape jobs")
    flags = await redis_conn_flags.lrange('scrape_sources', 0, -1)
    flags = [flag.decode('utf-8') for flag in flags]

    config_json = get_scraper_config()
    if not all(flag in config_json["scrapers"].keys() for flag in flags):
        raise HTTPException(status_code=400, detail="Invalid flags provided.")
    logger.info("Scrape jobs created")
    result = scrape_data_task.delay()
    logger.info(f"Scrape data task created with ID: {result.id}")
    return {"message": "Scraping triggered successfully."}
        

@app.get("/health")
def health_check():
    """Health check endpoint"""
    return {"status": "ok"}




./SSARE/nlp_service/Dockerfile:
FROM python:3.9

WORKDIR /app

# Copy the requirements.txt file
COPY nlp_service/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the core directory
COPY core ./core

# Copy the main.py and other files from postgres_service
COPY nlp_service/main.py .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "0420"]


./SSARE/nlp_service/main.py:
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from typing import List
from redis.asyncio import Redis
from core.models import ArticleBase
import json
from pydantic import ValidationError
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

"""
This Service runs on port 0420 and is responsible for generating embeddings for articles.
"""

app = FastAPI()

model = SentenceTransformer("all-MiniLM-L6-v2")

@app.post("/generate_embeddings")
async def generate_embeddings():
    """
    This function generates embeddings for articles that do not have embeddings.
    It is triggered by an API call from the orchestration container. 
    It reads from redis queue 5 - channel articles_without_embedding_queue.
    It writes to redis queue 6 - channel articles_with_embeddings.
    """
    try:
        redis_conn_raw = await Redis(host='redis', port=6379, db=5)
        redis_conn_processed = await Redis(host='redis', port=6379, db=6)

        # Retrieve articles from Redis Queue 5
        raw_articles_json = await redis_conn_raw.lrange('articles_without_embedding_queue', 0, -1)

        for raw_article_json in raw_articles_json:
            try:
                # Decode and load the article
                raw_article = json.loads(raw_article_json.decode('utf-8'))
                article = ArticleBase(**raw_article)

                # Generate embeddings
                embedding = model.encode(article.headline + " ".join(article.paragraphs)).tolist()
                article_with_embedding = article.model_dump()
                article_with_embedding["embeddings"] = embedding

                # Push to Redis Queue 6
                await redis_conn_processed.lpush('articles_with_embeddings', json.dumps(article_with_embedding))
            except (json.JSONDecodeError, ValidationError) as e:
                logger.error(f"Error processing article: {e}")

        return {"message": "Embeddings generated"}
    except Exception as e:
        logger.error(f"Error in generating embeddings: {e}")
        raise HTTPException(status_code=500, detail=str(e))



